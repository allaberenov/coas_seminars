# Представление данных

Поговорим о том, как данные хранятся в памяти.

## Целые числа

Минимально адресуемой единицей является байт. Сколько бит в байте? 

Ответ содержится в константе `CHAR_BIT` из `limits.h`. По стандарту гарантируется, что хотя бы 8. Однобайтовый тип данных называется `char`.

Типы, которыми вы привыкли пользоваться, такие как `int` плохи тем, что на разных системах занимают разное число байт (на 32-битных обычно 4, на 64-битных 8).
Почему так?

У любого процессора есть word size — это количество бит, с которыми он может работать за одну операцию (слово машины).

У 16-битного CPU слово = 16 бит → выгодно сделать int = 16 бит.

У 32-битного CPU слово = 32 бита → выгодно сделать int = 32 бита.

У 64-битного CPU слово = 64 бита. Какого размера int тут?

int всегда хотели связать с «быстрым» размером, чтобы арифметика была максимально эффективной.

Чтобы избежать всей этой путаницы, далее рекомендуется использовать типы с фиксированным числом бит: `int8_t`, `uint16_t` и так далее. Они определены в `stdint.h`.

Давайте с помощью небольшой демки (`print_int.c`) посмотрим на то, как эти числа хранятся в памяти.

Возьмём 8-битное беззнаковое число, как самый простой пример. В нём биты просто задают двоичное представление числа, начиная со старших разрядов.

На примере 8-битных знаковых мы видим, что первый бит отвечает за знак (0 для положительных и нуля, 1 для отрицательных).Для чисел от $0$ до $2^{n-1}$ (где $n$ это число знаков в двоичной записи)  представление совпадает с беззнаковыми. Для отрицательных представление получается из соображения, что $x+(-x)=2^n$ (старший бит отбрасывается из-за переполнения и получается просто 0, то есть мы как бы работаем в кольце вычетов по модулю $2^n$). 

Пример для -2 (3 бита):
```
  +2 = 010

  Инверсия → 101

  +1 → 110 == -2
```
Для чисел с большей разрядностью просто добавляются старшие байты в начало. `x86` поэтому называется `Little-endian` архитектурой. При передаче по сети наоборот принято передавать старшие байты в начале, можно привести число к такому виду с помощью функции `htons` и убедиться, что порядок изменился.

## UBSAN

Переполнение знакового типа является неопределённым поведением по стандарту. Чтобы это отлавливать, можно использовать флаг ``-fsanitize=undefined`` при компиляции (см. ``overflow.c``).

## Вещественные числа

Два основных типа вещественных с плавающей точкой, которые определены стандартом языка Си, - это `float` (используется 4 байта для хранения) и `double` (используется 8 байт).

Для простоты далее будет рассматриать `float` (см. ``print_float.c``).

<img width="590" height="75" alt="image" src="https://github.com/user-attachments/assets/e5875aa8-a5fc-4e8d-9f7c-eecd444932c0" />

На примере числа 5.3 разберём и перевод в двоичный, и упаковку в IEEE-754 float.

1) Разбиваем на целую и дробную части
* Целая часть: 5₁₀ = 101₂
* Дробная часть: 0.3₁₀ в двоичной — бесконечная периодическая дробь.

Как получить двоичную дробь (метод умножения на 2)

Берём 0.3 и много раз умножаем на 2, каждый раз фиксируя целую часть (0 или 1):
```
0.3 *2 = 0.6 → 0
0.6 *2 = 1.2 → 1 (остаётся 0.2)
0.2 *2 = 0.4 → 0
0.4 *2 = 0.8 → 0
0.8 *2 = 1.6 → 1 (остаётся 0.6)
0.6 *2 = 1.2 → 1 (остаётся 0.2)
... (дальше цикл 0.2→0.4→0.8→1.6→0.6→1.2→0.2 повторяется)


Отсюда:
0.3₁₀ = 0.01001100110011…₂ (период «0011»).
```
Значит всё число: 5.3₁₀ = 101.01001100110011…₂

2) Нормализуем (как требует IEEE-754)

```1.xxxxx × 2^n```

Сдвигаем «точку», чтобы слева была ровно одна 1:

101.0100110011…₂ = 1.010100110011…₂ × 2²

Порядок = 2

Дробная часть (после «1.») = 0.0100110011…₂

3) Поля IEEE-754 (для 32-битного float)

* Знак (S): 0 (число положительное)
* Смещённая экспонента (E): 127 + 2 = 129 = 10000001₂
* Мантисса (M): первые 23 бита после точки от 1.0100110011… Берём 23 бита с округлением до ближайшего (round to nearest, ties to even):

```
S |     E(8)     | M(23)
0 |   10000001   | 01010011001100110011010
```
Фактически это кодирует число:

(1 + 0.3250000476837158) × 2² = 5.300000190734863…


Т.е. ближайшее представимое float-значение:

точное хранимое значение: 5.300000190734863

погрешность: ≈ 1.9073486e-7 (относительная ≈ 3.6e-8)

Коротко: 5.3 нельзя представить точно в двоичной системе, поэтому float хранит ближайшее число, отсюда «хвостики» в выводе.



Видим, что старший бит вновь признак отрецательности числа. Следующие 8 бит называются экспонентой, а оставшиеся 23 - мантиссой.



Значение для обычных чисел (не денормализованных) может быть получено как:
      
<img width="277" height="52" alt="image" src="https://github.com/user-attachments/assets/4e1f5acf-3d3e-4487-89fb-42fb14174a35" />

где:
S - бит знака, если S=0 - положительное число; S=1 - отрицательное число
E - смещенная экспонента двоичного числа;
exp2 = E - (2(b-1) - 1) - экспонента двоичного нормализованного числа с плавающей точкой
(2^(b-1) + 1) - заданное смещение экспоненты (в 32-битном ieee754 оно равно +127 см.выше)
M - остаток мантиссы двоичного нормализованного числа с плавающей точкой

###Нормализованные и денормализованные числа в IEEE754

*** Нормализованные числа - это числа с плавающей точкой.
У них:

* экспонента ≠ 0 и ≠ max (например, 11111111 для float)
* мантисса хранит дробь с предполагаемой ведущей 1 (так называемая скрытая единица).

*** Денормализованные числа

Они нужны, чтобы числа очень близкие к нулю можно было хоть как-то представить.

У них:

* экспонента = 0,

* мантисса ≠ 0,

* нет скрытой единицы (ведущая цифра = 0).

Формула:

<img width="277" height="52" alt="image" src="https://github.com/user-attachments/assets/90e10f5b-4f09-4aca-8bd4-f85ace66f3b2" />

Диапазон чисел формата одинарной точности (32 бита) представленных по стандарту IEEE 754
<img width="700" height="400" alt="image" src="https://github.com/user-attachments/assets/c8f72b17-566a-4316-995f-51911e776140" />


Можно разбирать число с помощью ``union`` (см. ``print_union.c``).

> union в C — это специальный тип данных, где все поля занимают одно и то же место в памяти.
То есть у union все члены накладываются друг на друга, и размер union равен размеру самого большого поля.

> Визуализатор конвертация: https://www.h-schmidt.net/FloatConverter/IEEE754.html
> Визуализатор арифметики: http://weitz.de/ieee/
> Как производить арифметические действия: https://www.rfwireless-world.com/Tutorials/floating-point-tutorial.html

> Хороший материал по стандарту: http://www.softelectro.ru/ieee754.html
## Текстовые кодировки

![ASCII](ascii.png)

В середине 20 века возникла потребность как-то кодировать текст двоичными данными. Тогда решили, что 7 бит достаточно на цифры и латинский алфавит, так и появилась кодировка ASCII.

Позже один лишний бит позволил хранить в диапазоне от 128 до 255 символы национальных кодировок. В России, например, в 90-е была распространена `KOI8-R`.

Иметь отдельную кодировку под каждый язык всё ещё было не очень удобно, поэтому появилась `UTF-8`. Чтобы не занимать гарантированно 4 октета (будем называть так группы ), был придуман интересный дизайн. 

``echo 'C' | hexdump -C`` (`0a` отвечает за перевод строки, по сути есть один значащий байт `43` и код совпадает с ASCII)

То есть для самых частотных текстов, которые написаны латиницей, она занимает столько же место, сколько ASCII.

``echo 'Я' | hexdump -C`` (Байты `d0` `af` отвечают за `Я`)

Для большинства языков хватает двух байт.

``echo '😊' | hexdump -C`` 

4 байта нужно для каких-то специфических символов.

В общем случае кодирование подчиняется следующим шаблонам.

| Количество октетов | Шаблон |
|---|---|
|1|0xxxxxxx|
|2|110xxxxx 10xxxxxx|
|3|1110xxxx 10xxxxxx 10xxxxxx|
|4|11110xxx 10xxxxxx 10xxxxxx 10xxxxxx|
